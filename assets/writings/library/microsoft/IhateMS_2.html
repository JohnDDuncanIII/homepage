<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
<title>Why I hate Microsoft</title>
<meta http-equiv="Content-Type" content="text/html;charset=ISO-8859-1" />
<meta name="generator" content="vi" />
<meta name="robots" content="index, follow" />
<link rel="stylesheet" type="text/css" href="../../styles/general.css" />
<link rel="stylesheet" type="text/css" href="../../styles/content.css" />
<script language="JavaScript" type="text/javascript">
<!-- Hide
// Force frameset
if (parent.location.href == self.location.href) {
  document.write("<br />The document you have accessed is a subframe.\n");
  document.write("<a href=\"../../index.html\">Go here<\/a> for the main ");
  document.write("website.\n<hr />\n");
}
//  Unhide -->
</script>
<script type="text/javascript">
<!-- Hide
// Load and unload submenu
  function loadSubmenu() {
    parent.rightsubmenu.location.href = 'MSsubmenu.html';
  }
  function unloadSubmenu() {
    parent.rightsubmenu.location.href = '../../rightsubmenu.html';
  }
//  Unhide -->
</script>
<base target="_blank" />
</head>

<body onload="loadSubmenu();" onunload="unloadSubmenu();">

<img class="logo" src="MSimages/MSWD.jpg" alt="" width="110" height="80"
  border="0" align="left" />

<h1>Why I hate Microsoft</h1>
<h4>&quot;A personal, lengthy, but highly articulate outburst&quot;</h4>

<!-- Fifth revised edition -->

<br clear="all" />
<div align="right">
<a href="IhateMS.html" target="_self">Table Of Contents</a> |
<a href="IhateMS_1.html" target="_self">Previous chapter</a> |
<a href="IhateMS_3.html" target="_self">Next chapter</a>
</div>
<hr />

<h2>2. The not-so-good, the bad and the ugly</h2>

<blockquote>
  <p><i>&quot;... it is easy to be blinded to the essential uselessness of them
  by the sense of achievement you get from getting them to work at all. In
  other words ... their fundamental design flaws are completely hidden by their
  superficial design flaws.&quot;</i></p>
  <p align="right"><i>--&nbsp;The Hitchhiker's Guide to the Galaxy, on the
  products of the Sirius Cybernetics Corporation.</i></p>
</blockquote>

<p>Let's be honest: there's no such thing as bug-free software. Initial
versions of programs may occasionally crash, fail to de-allocate memory, or
encounter untested conditions. Developers may overlook security holes, users
may do things nobody thought of, and not all systems are identical. Software
developers are only human, and they make mistakes now and then. It happens.
But of all major software vendors Microsoft has <i>the worst record by far</i>
when it comes to the quality of their products in general.</p>

<h3>Outlining the battle field</h3>
<p>Microsoft boasts a rather extensive product range, but in fact there's
less here than meets the eye. Microsoft has forever been selling essentially
the same software over and over again, in a variety of colorful new
wrappers.</p>

<p>Microsoft products can be divided into three categories: applications,
operating systems, and additional server products. The applications include
the Microsoft Office suite, but also Internet Explorer, Media Player, Visio,
Frontpage, etc. The operating systems involve desktop and server versions of
Windows. On the desktop we find Windows 9x/ME, NT Workstation, Windows 2000,
XP and Vista, and at the server end we have Windows NT Server, Windows 2003
Server and varieties such as Datacenter. The additional server products,
e.g. Internet Information Server (IIS) and SQL Server, run on top of one
of the Windows server products. They add services (e.g.  webserver or
database server functions) to the basic file, print and authentication
services that the Windows server platform provides.</p>

<!-- The release date for the next Windows Server (after 2003) has been -->
<!-- announced as the first quarter of 2008, along with SQL Server 2008 -->

<h3>Two different Windows families</h3>
<p>Windows on the desktop comes in two flavors: the Windows 9x/ME product line,
and the Windows NT/2000/XP/Vista product line. The different versions within
one product line are made to look a bit different, but the difference is in the
details only; they are essentially the same. Windows '95, '98 and ME are
descended from DOS and Windows 3.x, and contain significant portions of old
16-bit legacy code. These Windows versions are essentially DOS-based, with
32-bit extensions. Process and resource management, memory protection and
security were added as an afterthought and are rudimentary at best. This
Windows product line is totally unsuited for applications where security
and reliability are an issue. It is completely insecure, e.g. it may ask for
a password but it won't mind if you don't supply one. There is no way to
prevent the user or the applications from accessing and possibly corrupting
the entire system (including the file system), and each user can alter the
system's configuration, either by mistake or deliberately. The Windows 9x/ME
line primarily targets consumers (although Windows '95 marketing was aimed
at corporate users as well). Although this entire product line was retired
upon the release of Windows XP, computers running Windows '98 or (to a
lesser degree) Windows ME are still common.</p>

<p>The other Windows product line includes Windows NT, 2000, XP and Vista,
and the server products. This Windows family is better than the 9x/ME line
and at least runs new (i.e. post-DOS) 32-bit code. Memory protection,
resource management and security are a bit more serious than in Windows
9x/ME, and they even have some support for access restrictions and a secure
filesystem. That doesn't mean that this Windows family is as reliable and
secure as Redmond's marketeers claim, but compared to Windows 9x/ME its
additional features at least have the advantage of being there at
all. But even this Windows line contains a certain amount of 16-bit legacy
code, and the entire 16-bit subsystem is a direct legacy from Microsoft's
OS/2 days with IBM. In short, all 16-bit applications share one 16-bit
subsystem (just as with OS/2). There's no internal memory protection, so
one 16-bit application may crash all the others and the the entire 16-bit
subsystem as well. This may create persistent locks from the crashed 16-bit
code on 32-bit resources, and eventually bring Windows to a halt. Fortunately
this isn't much of a problem anymore now that 16-bit applications have all
but died out.</p>

<p>While Windows has seen a lot of development over the years, relatively
little has really improved. The new features in new versions of Windows
all show the same half-baked, patchy approach. For each fixed problem,
at least one new problem is introduced (and often more than one). Windows
XP for example comes loaded with more applications and features than
ever before.  While this may seem convenient at first sight, the included
features aren't as good as those provided by external software. For
example, XP insists on supporting DSL (&quot;wideband Internet&quot;)
networking, scanners and other peripherals with the built-in Microsoft
code instead of requiring third-party code. So you end up with things
like DSL networking that uses incorrect settings (and no convenient
way to change that), scanner support that won't let you use your
scanner's photocopy feature, or a digital camera interface that will
let you download images from the camera but you can't use its webcam
function. Wireless (WiFi) network cards are even more of a problem:
where manufacturers could include their own drivers and client manager
software in previous versions of Windows, users are now redo to using
XP's native WiFi support. Unfortunately XP's WiFi support is full of
problems that cause wireless PCs to loose their connection to the wireless
access point with frustrating regularity. Also XP's native WiFi support
lacks extra functions (such as advanced multiple-profile management)
that manufacturers used to include in their client software.</p>

<p>Even basic services are affected. Windows 2000 and later have built-in
DNS (Domain Name System) caching. DNS is the mechanism that resolves
Internet host and domain names (e.g. www.microsoft.com) into the numerical
IP addresses used by computers (e.g. 194.134.0.67). Windows' DNS caching
basically remembers resolved hostnames for faster access and reduced DNS
lookups. This would be a nice feature, if it weren't for the blunder that
failed DNS lookups get cached by default as well. When a DNS lookup fails
(due to temporary DNS problems) Windows caches the unsuccessful DNS query,
and continues to fail to connect to a host regardless of the fact that
the DNS server might be responding properly a few seconds later. And of
course applications (such as Internet Explorer and Outlook) have been
integrated in the operating system more tightly than ever before, and
more (formerly separate) products have been bundled with the operating
system.</p>

<h3>Design flaws common to all Windows versions</h3>
<p>All versions of Windows share a number of structural design flaws.
Application installation procedures, user errors and runaway applications may
easily corrupt the operating system beyond repair. Networking support is poorly
implemented. Inefficient code leads to sub-standard performance, and both
scalability and manageability leave a lot to be desired. (See also
<a href="IhateMS_A.html" target="_self">appendix A</a>.) In fact, NT and its
successors (or any version of Windows) are just not comparable to the
functionality, robustness or performance that the UNIX community has been used
to for decades. They may work well, or they may not. On one system Windows
will run for weeks on end, on another it may crash quite frequently. I've
attended trainings at a Microsoft Authorized Education Center, and I was told:
&quot;We are now going to install Windows on the servers. The installation
will probably fail on one or two systems <i>[They had ten identical systems
in the classroom]</i> but that always happens - we don't know why and neither
does Microsoft.&quot; I repeat, this from a Microsoft Authorized Partner.</p>

<p>Be that as it may... Even without any installation problems or serious
crashes (the kind that require restore operations or reinstallations)
Windows doesn't do the job very well. Many users think it does, but
they generally haven't experienced any alternatives. In fact Windows'
unreliability has become commonplace and even proverbial; the dreaded blue
screen has featured in <a href="MSimages/bsod_cartoon.jpg">cartoons</a>,
<a href="http://www.winternals.com">screen savers</a> and on <a
href="MSimages/bluescreen.jpg">t-shirts</a>, it has appeared
at <a href="MSimages/bsod_airport.jpg">airports</a> and on <a
href="MSimages/bsod_videowall.jpg">buildings</a>, and there has even
been a Star Trek episode in which a malfunctioning space ship had to be
switched off and back on in order to get it going.</p>

<p>Even if Windows stays up it leaves a lot to be desired. On an
old-but-still-good desktop PC (e.g. a 450MHz PII CPU with 256MB RAM,
something we could only dream of fifteen years ago) four or five simultaneous
tasks are enough to tax Windows' multitasking capabilities to their limits,
even with plenty of core memory available. Task switches will start to take
forever, applications will stop responding simply because they're waiting for
system resources to be released by other applications (which may have halted
without releasing those resources), or kernel and library routines lock into
some unknown wait condition. Soon the whole system locks up entirely or
becomes all but unusable. In short, Windows' process management is as
unimpressive as its memory protection and resource management are,
and an operating system that may crash entirely when an application error
occurs should not be sold as a serious multi-tasking environment. Granted, it
does run several processes at once&nbsp;-&nbsp;but not very well. Recent
versions of Windows (i.e. XP and Vista) are better in this respect and more
stable than their predecessors, but not spectacularly so. Although they have
been patched up to reduce the impact of some of the most serious problems,
the basic flaws in the OS architecture remain; a crashing application (e.g.
a video player or a communications package) can still lock up the system or
throw it into a sudden and spontaneous reboot.</p>

<h3>Code separation, protection and sharing flaws</h3>
<p>Windows is quite fragile, and the operating system can get corrupted quite
easily. This happens most often during the installation of updates, service
packs, drivers or application software, and the problem exists in all versions
of Windows so far. The heart of the problem lies in the fact that Windows
can't (or rather, is designed not to) separate application and operating
system code and settings. Code gets mixed up when applications install
portions of themselves between files that belong to the operating system,
occasionally replacing them in the process. Settings are written to a
central registry that also stores vital OS settings. The registry database
is basically insecure, and settings that are vital to the OS or to other
applications are easily corrupted.</p>

<p>Even more problems are caused by the limitations of Windows' DLL subsystem.
A good multi-tasking and/or multi-user OS utilizes a principle called
<i>code sharing</i>. Code sharing means that if an application is running
<i>n</i> times at once, the code segment that contains the program code
(which is called the static segment) is loaded into memory only once,
to be used by <i>n</i> different processes which are therefore instances of
the same application. Apparently Microsoft had heard about something called
code sharing, but obviously didn't really understand the concept and the
benefits, or they didn't bother with the whole idea. Whatever the reason,
they went and used DLLs instead. DLL files contain Dynamic Link Libraries
and are intended to contain library functions only. Windows doesn't share
the static (code) segment - if you run 10 instances of Word, the bulk of
the code will be loaded into memory 10 times. Only a fraction of the
code, e.g. library functions, has been moved to DLLs and may be shared.</p>

<p>The main problem with DLL support is that the OS keeps track of DLLs by name
only. There is no adequate signature system to keep track of different DLL
versions. In other words, Windows cannot see the difference between one
WHATSIT.DLL and another DLL with the same name, although they may contain
entirely different code. Once a DLL in the Windows directory has been
overwritten by another one, there's no way back. Also, the order in which
applications are started (and DLLs are loaded) determines which DLL will
become active, and how the system will eventually crash. There is no
distinction between different versions of the same DLL, or between DLLs
that come with Windows and those that come with application software. An
application may put its own DLLs in the same directory as the Windows DLLs
during installation, and may overwrite DLLs by the same name if they exist.</p>

<p>What it boils down to is that the application may add portions of itself to
the operating system. (This is one of the reasons why Windows needs to be
rebooted after an application has been installed or changed.) That means that
<i>the installation procedure introduces third-party code (read: uncertified
code) into the operating system</i> and into other applications that load the
affected DLLs. Furthermore, because there is no real distinction between
system level code and user level code, the software in DLLs that has been
provided by application programmers or the user may now run at system level.
This corrupts the integrity of the operating system and other applications.
A rather effective demonstration was provided by Bill Gates himself who,
during a Comdex presentation of the Windows 98 USB Plug-and-Play features,
connected a scanner to a PC and caused it to crash into a Blue Screen.
&quot;Moving right along,&quot; said Gates, &quot;I guess this is why
we're not shipping it yet.&quot; Nice try, Mr. Gates, but of course the
release versions of Windows '98 and ME were just as unstable, and in Windows
2000 and its sucessors new problems have been introduced. These versions of
Windows use a firmware revision number to recognize devices, so an update of
a peripheral's firmware may cause that device to be 'lost' to PnP.</p>

<p>Another, less harmful but most annoying, side-effect of code confusion is
that different language versions of software may get mixed up. A foreign
language version of an application may add to or partially overwrite Windows'
list of dialog messages. This may cause a dialog window to prompt &quot;Are
you sure?&quot; in English, followed by two buttons marked, say,
&quot;Da&quot; and &quot;Nyet&quot;.</p>

<p>Peripheral drivers also use a rather weak signature system and suffer from
similar problems as DLL's, albeit to a lesser degree. For example, it's quite
possible to replace a printer driver with a similar driver from another
language version of Windows and mess up the page format as a result. Printer
drivers from different language versions of Windows sometimes contain entirely
different code that generates different printer output, but Windows is unaware
of the difference. This problem has been addressed somewhat with the release
of Windows 2000, but it's still far from perfect.</p>

<h3>Mixing up OS and application code</h3>
<p>Designing an OS to deliberately mix up system and application code fits
Microsoft's strategy of product bundling and integration. The results are
obvious: each file operation that involves executable code essentially puts
the entire OS and its applications at risk, and application errors often mean
OS errors (and crashes) as well. This leads to ridiculous &quot;issues&quot;
 such as Outlook Express crashing Windows if it's a &quot;high encryption&quot;
version with the locale set to France. Replying to an e-mail message may crash
the entire system, a problem which has been traced to one of the DLLs that
came with Outlook. (Are you still with me?)</p>

<p>In a well-designed and robustly coded OS something like this could never
happen.  The first design criterion for any OS is that the system, the
applications, and (in a multi-user environment) the users all be separated
and protected from each other. Not only does no version of Windows do that by
default, it actively prevents you from setting things up that way. The DLL
fiasco is just the tip of the iceberg. You can't maintain or adequately
restore OS integrity, you can't maintain version control, and you can't
prevent applications and users from interfering with each other and the
system, either by accident or on purpose.</p>

<h3>Beyond repair</h3>
<p>Then there's Windows' lack of an adequate repair or maintenance
mode. If <em>anything</em> goes wrong and a minor error or corruption
occurs in one of the (literally) thousands of files that make up Windows,
often the only real solution is a large-scale restore operation or
even to <em>reinstall</em> the OS. Yes, you read correctly. If your OS
suddenly, or gradually, stops working properly and the components which you
need to repair are unknown or being locked by Windows, the standard
approach to the problem (as recommended by Microsoft) is to do a complete
reinstallation. There's no such thing as single user mode or maintenance
mode to do repairs, nor is there a good way to find out which component has
been corrupted in the first place, let alone to repair the damage. (The
so-called 'safe mode' merely swaps configurations and does not offer
sufficient control for serious system repairs.)</p>

<p>Windows has often been criticized for the many problems that occur
while installing it on a random PC, which may be an A-brand or clone
system in any possible configuration. This criticism is not entirely
justified; after all it's not practically feasible to foresee all
the possible hardware configurations that may occur at the user
end. But that's not the point. The point is that these problems are
often impossible to fix or even properly diagnose, because most of
the Windows operating system is beyond the users' or administrators'
control. This is of course less true for Windows 9x/ME. Because these
are essentially DOS products, you can reboot the system using DOS and
do manual repairs to a certain degree. With Windows NT and its successors
this is generally impossible. Windows 2000, XP and Vista come
with an external repair console utility on the CD, that allows you some
access to the file system of a damaged Windows installation. But that's
about it.</p>

<p>The inability to make repairs has been addressed, to a certain degree, in
Windows XP. This comes with a 'System Restore' feature that tracks changes to
the OS, so that administrators may 'roll back' the system to a previous state
before the problem occurred. Also, the 'System File Check' feature attempts to
make sure that some 1000 system files are the ones that were originally
installed. If a &quot;major&quot; system file is replaced by another version
(for example if a Windows XP DLL file is overwritten by a Windows '95 DLL with
the same name) the original version will be restored. (Of course this also
prevents you from removing things like Outlook Express or Progman.exe, since
the specification of what is an important system file is rather sloppy.)
Windows Vista takes these features even further, by incorporating
transaction-based principles. This enhances the chances of a successful
roll-back from changes that have not been committed permanently yet.</p>

<p>Most of these workarounds are largely beyond the user's control. While
some of them may have adverse effects (e.g. File System Check may undo
necessary modifications) their effectivity is limited by nature. There
are many fault conditions possible that prevent automated repair features
from working correctly in the first place. When Windows breaks, the
automated features to recover from that fault generally break as well.
Also the number of faults that the automated repair options can deal with
are limited. At some point manual intervention is the only option, but
that requires the adequate maintenance mode that Windows doesn't have.
The inability of a commercial OS to allow for its own maintenance is a
good demonstration of its immaturity.</p>

<p>Even so, the added options for system restore in XP and Vista are an
improvement over the previous situation, in that at least a certain amount of
recovery is now possible. On the other hand, this illustrates Microsoft's
kludgy approach to a very serious problem: instead of implementing changes in
the architecture to prevent OS corruption, they perpetuate the basic design
flaw and try to deal with the damage after the fact. They don't fix the hole
in your roof, they sell you a bucket to put under it instead. When the bucket
overflows (i.e. the system recovery features are insufficient to solve a 
problem) you're still left with a mess.</p>

<h3>Wasted resources, wasted investments</h3>
<p>The slipshod design of Windows does not only reflect in its flawed
architecture. The general quality of its code leaves a lot to be desired
as well. This translates not only in a disproportionately large number of
bugs, but also in a lot of inefficiency. Microsoft needs at least three
or four times as much hardware to deliver the same performance that other
operating systems (e.g. Unix) deliver on much less. Likewise, on similar
hardware competing products perform much better, or will even run well
on hardware that does not meet Microsoft's minimum system requirements.</p>

<p>Inefficient code is not the only problem. Another issue is that
most bells and whistles in Microsoft products are expensive in terms of
additional hardware requirements and maintenance, but do not increase
productivity at all. Given the fact that ICT investments are expected to
pay off in increased productivity, reduced cost or both, this means that
most &quot;improvements&quot; in Microsoft products over the past decades
have been a waste of time from a Return On Investment standpoint. Typical
office tasks (e.g. accounting, data processing, correspondence) have
not essentially changed, and still take as much time and personpower as
they did in the MS-DOS era. However the required hardware, software and
ICT staff have increased manifold. Very few of these investments have
resulted in proportional increases in profit.</p>

<p>Only 32 kilobytes of RAM in the Apollo capsules' computers was enough
to put men on the moon and safely get them back to Earth. The Voyager
deep space probes that sent us a wealth of images and scientific data
from the outer reaches of the solar system (and still continue to do so
from interstellar space)  have on-board computers based on a 4-bit CPU. An
80C85 CPU with 176 kilobytes of ROM and 576 kilobytes of RAM was all that
controlled the Sojourner robot that drove across the surface of Mars and
delivered geological data data and high-resolution images in full-color
stereo. But when I have an 800MHz Pentium III with 256 Megabytes of RAM
and 40 Gigabytes of disk space, and I try to type a letter to my grandmother
using Windows XP and Office XP, the job will take me forever <em>because
my computer is underpowered!</em></p>

<p>Server-based or network-based computing is no solution either, mainly because
Windows doesn't have any real code sharing capability. If you were to shift
the workload of ten workstations to an application server (using Windows
Terminal Server, Citrix Server or another ASP-like solution) the server would
need a theoretical ten times the system resources of each of the workstations
it replaced to maintain the same performance, not counting the inevitable
overhead which could easily run up to an additional 10 or 20 percent.</p>

<p>Then there's the incredible amount of inefficient, or even completely
unnecessary code in the Windows file set. Take the 3D Pinball game in Windows
2000 Professional and XP Professional, for example. This game (you'll find it
under \Program&nbsp;Files\Windows&nbsp;NT\Pinball) is installed with Windows
and takes up a few megabytes of disk space. But most users will never know
that it's sitting there, wasting storage and doing nothing productive at all.
It doesn't appear in the program menu or control panel, and no shortcuts point
to it. The user isn't asked any questions about it during installation. In
fact its only conceivable purpose would be to illustrate Microsoft's
definition of 'professional'. No wonder Windows has needed more and more
resources over the years. A few megabytes doesn't seem much, perhaps, but
that's only because we've become used to the enormous footprints of Windows
and Windows applications. Besides, if Microsoft installs an entire pinball
game that most users neither need nor want, they obviously don't care about
conserving resources (which are paid for by the user community). What does
that tell you about the resource-efficiency of the rest of their code? Let me
give you a hint: results published in PC Magazine in April 2002 show that the
latest Samba software surpasses the performance of Windows 2000 by about
100 percent under benchmark tests. In terms of scalability, the results show
that Unix and Samba can handle four times as many client systems as Windows
2000 before performance begins to drop off.</p>

<p>Another example is what happened when one of my own clients switched from
Unix to Windows (the reason for this move being the necessity to run some
webbased accounting package with BackOffice connectivity on the server).
Their first server ran Unix, Apache, PHP and MySQL and did everything it had
to do with the engine barely idling. On the same system they then installed
Windows Server 2003, IIS, PHP and MySQL, after which even the simplest of
PHP scripts (e.g. a basic 100-line form generator) would abort when the 30
second execution timeout was exceeded.</p>

<p>Paradoxically, though, the fact that Microsoft products need humongous piles
of hardware in order to perform decently has contributed to their commercial
success. Many integrators and resellers push Microsoft software because it
enables them to prescribe the latest and heaviest hardware platforms in the
market. Unix and Netware can deliver the same or better performance on much
less. Windows 2000 and XP however need bigger and faster systems, and are
often incompatible with older hardware and firmware versions (especially the
BIOS). This, and the fact that hardware manufacturers discontinue support for
older hardware and BIOSes, forces the user to purchase expensive hardware with
no significant increase in return on investment. This boosts hardware sales,
at the expense of the &quot;dear, valued customer&quot;. Resellers make more
money when they push Microsoft products. It's as simple as that.</p>

<h3>Many small flaws make a big one</h3>
<p>Apart from the above (and other) major flaws there's also a staggering
amount of minor flaws. In fact there are so many minor flaws that their sheer
number can be classified as a major flaw. In short, the general quality of
Microsoft's entire set of program code is sub-standard. Unchecked buffers,
unverified I/O operations, race conditions, incorrectly implemented protocols,
failures to deallocate resources, failures to check environmental parameters,
et cetera ad nauseam... You name it, it's in there. Microsoft products contain
some extremely sloppy code and bad coding practices that would give an
undergraduate some well-deserved bad marks. As a result of their lack of
quality control, Microsoft products and especially Windows are riddled with
literally thousands and thousands of bugs and glitches. Even many of the
error messages are incorrect!</p>

<p>Some of these blunders can be classified as clumsy design rather than
as mere sloppiness. A good example is Windows' long filename support. In
an attempt to allow for long filenames in Windows '9x/ME, Microsoft
deliberately broke the FAT file system.  They stored the extension
information into deliberately cross-linked directory entries, which is
probably one of their dirtiest kludges ever. And if that wasn't enough,
they made it legal for filenames to contain whitespace. Because this was
incompatible with Windows' own command line parsing (Windows still expects
the old FAT notation) another kludge was needed, and whitespace had to
be enclosed in quotation marks. This confused (and broke) many programs,
including many of Microsoft's own that came with Windows.</p>

<p>Another good example is Windows' <i>apparent</i> case-sensitivity. Windows
<i>seems</i> to make a distinction between upper and lower case when handling
filenames, but the underlying software layers are still case-insensitive. So
Windows only changes the case of the files and directories as they are
presented to the user. The names of the actual files and directories may be
stored in uppercase, lowercase or mixed case, while they are still presented
as capitalized lower case file names. Of course this discrepancy causes no
problems in a Windows-only environment. Since the underlying code is
essentially case-insensitive, case is not critical to Windows' operation.
However as soon as you want to incorporate Unix-based services (e.g. a
Unix-based webserver instead of IIS) you discover that Windows has messed
up the case of filenames and directories.</p>

<p>But most of Windows' errors and glitches are just the result of sloppy work.
Of course there is no such thing as bug-free software, but the amount of bugs
found in Windows is, to put it mildly, disproportionate. For example, Service
Pack 4 for Windows NT 4.0 attempted to fix some 1200 bugs (yes, one thousand
two hundred). But there had already been three previous service packs at the
time! Microsoft shamelessly admitted this, and even boasted about having
&quot;improved&quot; NT on 1200 points. Then they had to release several
more subsequent service packs in the months that followed, to fix remaining
issues and of course the additional problems that had been introduced by the
service packs themselves.</p>

<p>An internal memo among Microsoft developers mentioned 63,000 (yes: sixty-three
<em>thousand</em>) known defects in the initial Windows 2000 release. Keith
White, Windows Marketing Director, did not deny the existence of the document,
but claimed that the statements therein were made in order to &quot;motivate
the Windows development team&quot;. He went on to state that &quot;Windows
2000 is the most reliable Windows so far.&quot; Yes, that's what he said.
A product with 63,000 known defects (mind you, that's only the <i>known</i>
defects) and he admits it's the best they can do. Ye gods.</p>

<p> And the story continues: Windows XP Service Pack 2 was touted to address
a large number of security issues and make computing safer. Instead it breaks
many things (mostly products made by Microsoft's competitors, but of course
that is merely coincidence) but does not really fix any real security flaws.
The first major security hole in XP-SP2 was described by security experts as
&quot;not a hole but rather a crater&quot; and allowed downloadable code
to spoof firewall information. Only days after XP-SP2 was released the first
Internet Explorer vulnerability of the SP2-era was discovered. Furthermore
SP2 leaves many unnecessary networking components enabled, bungles
permissions, leaves IE and OE open to malicious scripts, and installs a packet
filter that lacks a capacity for egress filtering. It also makes it more
difficult for third-party products (especially multimedia plugins) to access
the ActiveX controls, which in turn prevents the installation of quite a
bit of multimedia software made by Microsoft's competitors. XP-SP2's most
noticeable effect (apart from broken application compatibility) are frequent
popups that give the user a <i>sense</i> of security. Apart from this placebo
effect the long-awaited and much-touted XP-SP2 doesn't really fix very
much.</p>

<p>In the summer of 2005 Jim Allchin, then group VP in charge of Windows,
finally went and admitted all this. In a rare display of corporate
honesty, he told the Wall Street Journal that the first version of
Longhorn (then the code name for Windows Vista) had to be entirely
scrapped because the quality of the program code had deteriorated too
far. The root of the problem, said Allchin, was Microsoft's historical
approach to developing software (the so-called &quot;spaghetti code
culture&quot;) where the company's thousands of programmers would each
develop their own piece of code and it would then all be stitched together
at the end. Allchin also said to have faced opposition to his call for
a completely new development approach, firstly from Gates himself and
then the company's engineers.</p>

<h3>MS developers: &quot;We are morons&quot;</h3>
<p>Allchin's revelations came as no great surprise. Part of the source
code to Windows 2000 had been leaked onto the Internet before, and pretty
it was not. Microsoft's flagship product turned out to be a vast sprawl of
spaghetti in Assembly, C and C++, all held together with sticky tape and paper
clips. The source code files contained many now-infamous comments including
&quot;We are morons&quot; and &quot;If you change tabs to spaces, you will
be killed!  Doing so f***s the build process&quot;.</p>

<p>There were many references to idiots and morons, some external but
mostly at Microsoft. For example: </p>

<ul>
<li>In the file <tt>private\ntos\rtl\heap.c</tt>, which dates from 1989:<br />
    // The specific idiot in this case is Office95, which likes<br />
    // to free a random pointer when you start Word95 from a desktop<br />
    // shortcut.<br /><br /></li>
<li>In the file <tt>private\ntos\w32\ntuser\kernel\swp.c</tt> from
       11-Jul-1991:<br>
    // for idiots like MS-Access 2.0 who SetWindowPos( SWP_BOZO )<br / >
    // and blow away themselves on the shell, then lets<br />
    // just ignore their plea to be removed from the tray.<br /><br /></li>
<li>Morons are also to be found in the file
       <tt>private\genx\shell\inc\prsht.w</tt>:<br />
    // We are such morons. Wiz97 underwent a redesign between IE4 and
       IE5<br /><br /></li>
<li>And in <tt>private\shell\shdoc401\unicpp\desktop.cpp</tt>:<br />
    // We are morons. We changed the IDeskTray interface between
       IE4<br /><br /></li>
<li>In <tt>private\shell\browseui\itbar.cpp:</tt><br />
    // should be fixed in the apps themselves. Morons!<br /><br /></li>
<li>As well in <tt>private\shell\ext\ftp\ftpdrop.cpp</tt>:<br />
    We have to do this only because Exchange is a moron.</li>
</ul>

<p>Microsoft programmers also take their duty to warn their fellow developers
seriously against unsavory practices, which are apparently committed on a 
regular basis. There are over 4,000 references to &quot;hacks&quot;.
These include:</p>

<ul>
<li>In the file <tt>private\inet\mshtml\src\core\cdbase\baseprop.cxx</tt>:<br />
    // HACK! HACK! HACK! (MohanB) In order to fix #64710<br>
    // at this very late date<br /><br /></li>
<li>In <tt>private\inet\mshtml\src\core\cdutil\genutil.cxx</tt>:<br />
    // HACK HACK HACK. REMOVE THIS ONCE MARLETT IS AROUND<br /><br /></li>
<li>In <tt>private\inet\mshtml\src\site\layout\flowlyt.cxx</tt>:<br />
    // God, I hate this hack ...<br /><br /></li>
<li>In <tt>private\inet\wininet\urlcache\cachecfg.cxx</tt>:<br />
    // Dumb hack for back compatibility. *sigh*<br /><br /></li>
<li>In <tt>private\ispu\pkitrust\trustui\acuictl.cpp</tt>:<br />
    // ACHTUNG! HACK ON TOP OF HACK ALERT:<br />
    // Believe it or not there is no way to get current height<br /><br /></li>
<li>In <tt>private\ntos\udfs\devctrl.c</tt>:<br />
    // Add to the hack-o-rama to fix formats.<br /><br /></li>
<li>In <tt>private\shell\shdoc401\unicpp\sendto.cpp</tt>:<br />
    // Mondo hackitude-o-rama.<br /><br /></li>
<li>In <tt>private\ntos\w32\ntcon\server\link.c</tt>:<br />
    // HUGE, HUGE hack-o-rama to get NTSD started on this
       process!<br /><br /></li>
<li>In <tt>private\ntos\w32\ntuser\client\dlgmgr.c</tt>:<br />
    // HACK OF DEATH!!<br /><br /></li>
<li>In <tt>private\shell\lib\util.cpp</tt>:<br />
    // TERRIBLE HORRIBLE NO GOOD VERY BAD HACK<br /><br /></li>
<li>In <tt>private\ntos\w32\ntuser\client\nt6\user.h</tt>:<br />
    // The magnitude of this hack compares favorably with that<br />
    // of the national debt.</li>
</ul>

<p>The most worrying aspect here is not just how these bad practices
persist and even find their ways into release builds in large
numbers. After all, few things are as permanent as a &quot;temporary&quot;
solution. Nor is it surprising how much ancient code still exists in
the most recent versions of Windows (although it is somewhat unsettling
to see how decades-old mistakes continue to be a problem). No, the
most frightening thing is that Microsoft's developers obviously know
they are doing terrible things that serious undermine the quality
of the end product, but <i>are apparently unable to remedy the known bad
quality of their own code</i>.</p>

<p>As you may remember, Windows XP was already out by the time that the above
source code got leaked. In fact, back in 2004, Microsoft had been talking
about Longhorn (Windows Vista) for three years. Just a few months after the
source code leaked out, it was announced that WinFS, touted as Microsoft's
flagship Relational File System Of The Future, would not ship with Vista
after all. The reason isn't hard to guess: Windows' program code has become
increasingly unmaintainable and irrepairable over the years.</p>

<p>In the long years since XP was launched, Apple have come out with five
major upgrades to OSX, upgrades which (dare I say it?) install with about
as much effort as it takes to brush your teeth in the morning. No nightmare
calls to tech-support, no sudden hardware incompatibilities, no hassle. Yet
Microsoft has failed to keep up, and the above example of the state of their
program code clearly demonstrates why.</p>

<h3>Unreliable servers</h3>
<p>All these blunders have of course their effects on Windows' reliability
and availability. Depending on application and system load, most Windows
systems tend to need frequent rebooting, either to fix problems or on a
regular basis to prevent performance degradation as a result of Windows'
shaky resource management.</p>

<p>On the desktop this is bad enough, but the same flaws exist in the Windows
server products. Servers are much more likely to be used for mission-critical
applications than workstations are, so Windows' limited availability and its
impact on business become a major issue. The uptimes of typical Windows-based
servers in serious applications (i.e. more than just file and print services
for a few workstations) tend to be limited to a few weeks at most. One or
two server crashes (read: interruptions of business and loss of data) every
few months are not uncommon. As a server OS, Windows clearly lacks
reliability.</p>

<p>Windows server products aren't even really server OSes. Their architecture is
no different from the workstation versions. The server and workstation
kernels in NT are identical, and changing two registry keys is enough to
convince a workstation that it's a server. Networking capabilities are still
largely based on the peer-to-peer method that was part of Windows for
Workgroups 3.11 and that Microsoft copied, after it had been successfully
pioneered by Apple and others in the mid-eighties. Of course some code in the
server products has been extended or optimized for performance, and
domain-based authentication has been added, but that doesn't make it a true
server platform. Neither does the fact that NT Server costs almost three times
as much as NT Workstation. In fact we're talking about little more than Windows
for Workgroups on steroids.</p>

<p>In November 1999, Sm@rt Reseller's Steven J. Vaughan-Nichols ran a test
to compare the stability of Windows NT Server (presumably running Microsoft
Internet Information Server) with that of the Open Source Linux operating
system (running Samba and Apache). He wrote:</p>

<blockquote><i>
  Conventional wisdom says Linux is incredibly stable. Always skeptical,
  we decided to put that claim to the test over a 10-month period. In our
  test, we ran Caldera Systems OpenLinux, Red Hat Linux, and Windows NT
  Server 4.0 with Service Pack 3 on duplicate 100MHz Pentium systems with
  64MB of memory. Ever since we first booted up our test systems in January,
  network requests have been sent to each server in parallel for standard
  Internet, file and print services. The results were quite revealing. Our
  NT server crashed an average of once every six weeks. Each failure took
  roughly 30 minutes to fix. That's not so bad, until you consider that
  <b>neither Linux server ever went down</b>.
</i></blockquote>

<p>Interesting: a crash that takes 30 minutes to fix means that something
critical has been damaged and needs to be repaired or restored. At least it
takes more than just a reboot. This happens once every six weeks <i>on a
server</i>, and that's considered &quot;not so bad&quot;... Think about
it. Also note that most other Unix flavors such as Solaris, BSD or AIX are
just as reliable as Linux.</p>

<p>But the gap between Windows and real uptime figures is even greater than
Vaughan-Nichols describes above. Compare that half hour downtime per six
weeks to that of Netware, in the following article from Techweb on 9 April
2001:</p>

<blockquote><i>
  Server 54, Where Are You?<br />
  The University of North Carolina has finally found a network server that,
  although missing for four years, hasn't missed a packet in all that time.
  Try as they might, university administrators couldn't find the server.
  Working with Novell Inc. (stock: NOVL), IT workers tracked it down by
  meticulously following cable until they literally ran into a wall. The
  server had been mistakenly sealed behind drywall by maintenance workers. 
</i></blockquote>

<p>Although there is some doubt as to the actual truth of this story, it's a
known fact that Netware servers are capable of years of uninterrupted
service. Shortly before I wrote this, I brought down a Netware server at our
head office. This was a Netware 5.0 server that also ran software to act as
the corporate SMTP/POP3 server, fax server and main virus protection for the
network, next to providing regular file and print services for the whole
company. It had been up and running without a single glitch for more than a
year, and the only reason we shut it down was because it had to be physically
moved to another building. Had the move not been necessary, it could have run
on as long as the mains power held out. There's simply no reason why its
performance should be affected, as long as nobody pulls the plug or rashly
loads untested software. The uptimes of our Linux and Solaris servers
(mission-critical web servers, database servers and mail servers, or just
basic file and print servers) are measured in months as well. Uptimes
in excess of a year are not uncommon for Netware and Unix platforms, and
uptimes of more than two years are not unheard of either. Most OS updates
short of a kernel replacement do not require a Unix server to be rebooted,
as opposed to Windows that expects a complete server reboot whenever a DLL
in some subsystem is updated. But see for yourself: check the
<a href="http://www.netcraft.com">Netcraft Uptime</a> statistics and
compare the uptimes of Windows servers to those of Unix servers. The figures
speak for themselves.</p>

<p>Microsoft promises 99.999% availability with Windows 2000. That's a little
over 5 minutes of downtime per year. Frankly I can't believe this is a
realistic target for Windows. Microsoft products have never even approached
such uptime figures. Even though most of the increased availability of
Windows 2000 must be provided through third-party clustering and redundancy
solutions (something that the glossy ads neglect to mention) it's highly
unlikely that less than five minutes of downtime per year for the entire
Windows cluster is practically feasible.</p>

<p>Perhaps even more serious is the fact that, short of clustering, there is no
adequate solution for the many software glitches that jeopardize the
availability of a typical Windows server. A typical NT or 2000 server can
spontaneously develop numerous transient problems. These may vary from network
processes that seem healthy but ignore all network requests, to runaway
server applications that lock up the entire operating system. Usually the only
solution in these cases is to power cycle and restart the system. I remember
having to do that three times a week on a production server. Believe me, it's
no fun. Perhaps it's understandable that some network administrators feel
that the best way to accelerate a Windows system is at 9.81 meters per second
squared.</p>

<h3>More worries, more cost, or both</h3>
<p>Does all this make Windows an entirely unusable product that cannot run in
a stable configuration anywhere? No, fortunately not. There are situations 
where Windows systems (both workstations and servers) may run for long
periods without crashing. A vendor-installed version of Windows NT of 2000 on
an HCL-compliant, A-brand system, with all the required service packs and
only certified drivers, should give you relatively few problems (provided
that you don't use it for much more than basic file and print services, of
course). The rule of thumb here is to use only hardware that is on Microsoft's
Hardware Compatibility List (HCL), to use only vendor-supplied, certified
drivers and other software, and to use third-party clustering solutions for
applications where availability is a critical issue.</p>

<p>Another rule of thumb is: one service, one server. Unix sysadmins would
expect to run multiple services on one server and still have resources to
spare. Good Windows sysadmins generally don't do that. If you need to run a
file/print server, a web server and a mail server, all under Windows, use
three servers. This will minimize the risk of software conflicts, and it will
help prevent overload. On the other hand, you now have to maintain three
servers instead of one, which in turn requires more IT staff to keep up with
the work.</p>

<p>A diligent regime of upgrading and running only the latest versions of
Microsoft products may help as well. Such a policy will cost a small fortune
in license upgrades, but it may help to solve and even prevent some problems.
To be honest, Windows 2000 and XP on the desktop, and Windows Server 2003
in the server room, are somewhat better (or rather, less bad) than NT4
was. These versions are at least more stable, and less prone to spontaneous
crashes, than NT4 was. Some of NT's most awkward design blunders have been
fixed. For example, the user home directories are no longer located under the
WINNT directory. On most systems (especially on notebook computers) 2000 and
XP are  considerably less shaky, and hardware support is certainly better.
Which goes to show that a few relatively trivial changes may go a long way, I
suppose.</p>

<p> But still, given the general quality of Microsoft products and Windows in
particular, there are absolutely no guarantees. And of course Microsoft
introduced a whole new set of glitches and bugs in Windows XP, which largely
undid many of the improvements in Windows 2000. So now Windows XP is less
stable in some situations than Windows 2000 was. But that's innovation for
you, I suppose.</p>

<h3>Denial will see us through</h3>
<p>One frightening aspect about all this is that Microsoft doesn't seem
to realize how serious these problems are. Or rather, they probably realize it
but they don't seem to care as long as sales hold up. While the core systems
of large companies still run on either mainframes or midrange Linux systems
in order to provide sufficient reliability and performance, Microsoft sales
reps pretend that Windows is good enough to compete in that area.</p>

<p>Microsoft likes to pretend that Windows' huge shortcomings are only minor.
Their documents on serious problems (which are always called 'Issues' in
Microsoft-speak) are very clear on that point. Take the classic 'TCP/IP Denial
Of Service Issue' for example: a serious problem that was discovered a few
years ago. It caused NT servers to stop responding to network service requests,
thus rendering mission-critical services unavailable. (This should not be
confused with deliberate Denial Of Service attacks to which most operating
systems are vulnerable; this was a Windows issue only.) At the time there was
no real solution for this problem. Microsoft's only response at the time was
to state that <cite>&quot;This issue does not compromise sensitive data in
any way. It merely forces a server to become unavailable for a short time,
which is easily remedied by rebooting the server.&quot;</cite> NT sysadmins
had to wait for the next service pack that was released several months later
before this problem was addressed. In the meantime they were expected to
accept downtime and the rebooting of mission-critical servers as a matter of
course. After all no data was lost, so how bad could it be?</p>

<p>And Microsoft thinks that this stuff can compete with Unix and threaten the
mainframe market for mission-critical applications?<br />
Uh-huh. I don't think so.</p>

<p>In September 2001 Hewlett-Packard clustered 225 PCs running the Open Source
Linux operating system. The resulting system (called I-cluster) benchmarked
itself right into the global top-500 of supercomputers, using nothing but
unmodified, out-of-the-box hardware. (A significant number of entries in that
top-500, by the way, runs Linux, and more and more Unix clusters are being
used for supercomputing applications.) Microsoft, with a product line that is
descended solely from single-user desktop systems, can't even dream of such
scalability&nbsp;-&nbsp;not now, not ever. Nevertheless Microsoft claimed
on a <a href="http://www.wehavethewayout.com">partner website</a> with Unisys
that Windows will outperform Unix, because Unisys' server with Windows 2000
Datacenter could be scaled up to 32 CPU's. This performance claim is of course
a blatant lie: the opposite is true and they know it. Still Microsoft would
have us believe that the performance, reliability and scalability of the
entire Windows product line is on par with that of Unix, and that clustered
Windows servers are a viable replacement option for mainframes and Unix
midrange systems. I'm not kidding, that's what they say. If you're at all
familiar with the scalability of Unix midrange servers and the requirements of
the applications that mainframes are being used for, you will realize how
ludicrous this is.</p>

<h3>Microsoft lacks confidence in own products</h3>
<p>Dog food is sold to the owners who buy it, not to the dogs who have to eat
it. &quot;Eating your own dog food&quot; is a metaphor for a programmer who
uses the system he or she is working on. Is it yet functional enough for real
work? Would you trust it not to crash and lose your data? Does it have rough
edges that scour your hand every time you use a particular feature? Would you
use it yourself by choice?</p>

<p>When Microsoft acquired the successful Hotmail free Email service, the
system had roughly 10 million users, and the core systems that powered
Hotmail all ran Unix. A few years later the number of Hotmail users exceeds 100
million, but in spite of Microsoft's claims about the power of Windows 
and their previous intentions to replace Hotmail's core systems with Windows
servers, Hotmail's core systems still run Unix. This was discussed thoroughly
in a leaked-out <a href="http://www.securityoffice.net/mssecrets/hotmail.html"
  target="_blank">internal paper</a> by Microsoft's Windows 2000 Server
Product Group member David Brooks. It mentioned the proverbial stability of the
Unix kernel and the Apache web server, the system's transparency and
combination of power and simplicity. Windows on the other hand it considered
to be needlessly GUI-biased (Brooks wrote: &quot;Windows [...] server products
continue to be designed with the desktop in mind&quot;) and also complex,
obscure and needlessly resource-hungry. (Brooks: &quot;It's true that Windows
kequires a more powerful computer than Linux or FreeBSD [and treats a server]
reboot as an expectation&quot;.)</p>

<p>Hotmail is not the only example of Microsoft's refusal to eat their
own dog food. The &quot;We have the way out&quot; anti-Unix website that
Microsoft (along with partner Unisys) put up in the first months of 2002,
initially ran Unix and Apache. (It was ported to IIS on Windows 2000 only
after the entire ICT community had had a good laugh). For years Microsoft's
own email servers have been protected by third-party security software, which
amounts to a recognition of the fact that Exchange on Windows needs such
third party assistance to provide even a basic level of system security.<br />
Microsoft's SQL Labs, the part of the company that works on Microsoft's SQL
Server, purchased NetScreen's 500-series security appliance to defend its
network against Code Red, Nimda and other worm attacks. Apparently the labs'
choice was made despite the fact that Microsoft then already sold its own
security product touted as a defense against such worms. The Microsoft ISA
[Internet Security and Acceleration] Server was introduced in early 2001 and
was hailed by Microsoft as their first product aimed entirely at the security
market. In fact, the most important reason businesses ought to switch to ISA
Server, according to Microsoft, was that &quot;ISA Server is an [...]
enterprise firewall and secure application gateway designed to protect the
enterprise network from hacker intrusion and malicious worms&quot;. Still
Microsoft's SQL Labs prudently decided to rely on other products than their
own to provide basic security.<br />
And Microsoft's own accounting division used IBM's AS/400 midrange platform
for critical applications such as the payroll system, until well in the late
nineties.<br />
Food for thought.</p>

<h3>Network pollution</h3>
<p>It should also be mentioned that Microsoft doesn't know the first thing
about networking. A Windows system in a TCP/IP environment still uses a
NetBIOS name. Microsoft networking is built around NetBEUI, which is an
extended version of NetBIOS. This is a true Stone Age protocol which is
totally unroutable. It uses lots of broadcasts, and on a network segment
with Windows PCs broadcasts indeed make up a significant portion of the
network traffic, even for point-to-point connections (e.g. between a Microsoft
mailbox and a client PC). If it weren't for the fact that it is possible to
encapsulate NetBIOS/NetBEUI traffic in a TCP/IP envelope, connecting Windows 
to the real world would be totally impossible. (Note that Microsoft calls the
IP encapsulation of NetBEUI packets 'native IP'. Go figure.) The problem
is being made worse by the ridiculous way in which Microsoft applications
handle file I/O. Word can easily do over a hundred 'open' operations on
one single file, and saving a document involves multiple write commands
with only one single byte each. Thus Windows PCs tend to generate an inordinate
amount of garbage and unnecessary traffic on the network.</p>

<p>Microsoft's design approach has never shown much understanding of of computer
networking. I remember reading a document from Microsoft that stated that a
typical PC network consists of ten or at most twenty peer-to-peer workstations
on a single cable segment, all running Microsoft operating systems. And that
explains it, I suppose. If you want anything more than that, on your own
head be it.</p>

<p>Here's a simple test. Take a good, fast FTP server (i.e. one that runs
on Unix). Upload and download a few large files (say, 50MB) from and to a
Windows NT or 2000 workstation. (I used a 233MHz Pentium-II.) You will
probably see a throughput in the order of 1 Mbit/s for uploads and 2 to 3
Mbit/s for downloads, or more on faster hardware.<br />
Then boot Linux <em>on the same workstation</em> (a quick and easy way is to
use a Linux distribution on a ready-to-run CD that requires no installation,
such as <a href="http://www.knopper.net/knoppix/index-en.html">Knoppix</a>).
Then repeat the upload and download test. You will now see your throughput
limited only by the bandwidth or your network connection, the capacity
of your FTP server, or by your hardware performance, whichever comes first.
On 10 Mbit/s Ethernet, 5 Mbit/s upload <em>and</em> download throughput are
the least you may expect. To further test this, you can repeat it with a
really slow client (e.g. a 60 or 75MHz Pentium) running Linux. The throughput
limit will still be network-bound and not system-bound. (Note: this is not
limited to FTP but also affects other network protocols. It's a performance
problem related to the code in Windows' IP stack and other parts of the
architecture involved with data throughput.)</p>

<p>New Windows versions bring no relief here. Any network engineer who uses
PPPoE (Point-to-Point Protocol over Ethernet) with ADSL will tell you that the
MTU (a setting that limits packet size) should be set to 1492 or less. In XP
it's set by default to 1500, which may lead to problems with the routers of
many DSL ISPs. Microsoft is aware of the problem, but XP nevertheless persists
in setting up PPPoE with an MTU of 1500. There is a registry hack for PPPoE
users, but there is no patch, and XP has no GUI-based option which enables the
user to change the MTU conveniently.</p>

<p>The above example is fairly typical of XP. It tries to do things itself
and botches the job, rather than give you control over it to do it properly.
But all versions of Windows share a number of clumsily designed and coded
network features, starting with Windows file sharing. This service uses
fixed ports, and can't be moved to other ports without using dynamite. This
means that routing the essentially insecure Windows file sharing connections
through a secure SSH tunnel is extremely cumbersome, and requires disabling
(or, on XP, uninstalling) file sharing services on the local client, so that
using both a tunneled and a non-tunneled file sharing connection at the
same time is impossible, and switching back and forth between the two requires
rebooting. Yes, you could conceivably solve this with a VPN configuration,
but that's not the point. The point is that any self-respecting network client
will let you configure the ports it uses but, apparently for reasons of
user-friendliness, Microsoft hard-coded the file sharing ports into their
software, thereby making it impossible to extend file sharing beyond insecure
connections on a local LAN.</p>

<p>While many Windows' networking limitations are rapidly phasing out now that
the '9x/ME product line has been abandoned, others persist. Set an XP or
Vista box or a Windows 2003 server to share files, and then try to get
Windows networking to 'see' those shares over a VPN or from the other end
of an Internet router. You can't, or at least not without cumbersome and
unnecessarily expensive workarounds, due to Windows Networking still being
based on a non-routable IBM protocol from the 1970's.</p>

<p>On top of all this, readers of this paper report that according to John
Dvorak in PC Magazine, the Windows kernel maxes out at 483 Mbps. He remarks
that as many businesses are upgrading to 1 Gigabit Ethernet, Windows
(including XP) just can't keep up.<br />
Now go read what Microsoft writes about Windows 2000 and XP being <em>the</em>
ideal platform for Internet applications...</p>

<h3>Denial of Service vulnerabilities</h3>
<p>The sloppy nature of Windows' networking support code and protocol stacks
also makes the system more vulnerable to Denial of Service attacks. A DoS
attack is a form of computer vandalism or sabotage, with the intention to
crash a system or otherwise render it unavailable. In a typical DoS attack
a deliberately malformed network packet is sent to the target system, where
it triggers a known flaw in the operating system to disrupt it. In the case
of Windows, though, there are more ways to bring down a system. For example,
the kernel routines in Windows 2000 and XP that process incoming IPsec (UDP
port 500) packets are written so badly that sending a stream of regular IPsec
packets to the server will cause it to bog down in a CPU overload. And of
course Windows' IPsec filters cannot block a 500/udp packet stream.</p>

<p>Another way to render a system unavailable is a Distributed Denial of
Service attack. A DDoS attack involves many networked systems that send network
traffic to a single target system or network segment, which is then swamped
with traffic and becomes unreachable. There's very little that can be done
against DDoS attacks, and all platforms are equally vulnerable.</p>

<p>With all these DoS and DDoS vulnerabilities, it's a worrying development
that Windows 2000 and XP provide new platforms to generate such attacks. The
only real 'improvement' in Windows 2000's and XP's IP stacks is that for no
good reason whatsoever, Microsoft has extended the control that an application
has over the IP stack. This does not improve Windows' sub-standard networking
performance, but it gives applications the option to build custom IP packets
to generate incredibly malicious Internet traffic. This includes spoofed
source IP addresses and SYN-flooding full scale Denial of Service (DoS)
attacks. As if things weren't bad enough...</p>
<!-- Source: Gibson Research, www.grc.com -->

<h3>Cumulative problems on the server</h3>
<p>So far we have concentrated on Windows. Most of the problems with Microsoft
products originate here, since Windows is by far the most complex Microsoft
product line, and there are more interactions between Windows and other
products than anywhere else. But unfortunately most server and desktop
applications are cut from the same cloth as Windows is. The general quality
of their code and design is not much better.</p>

<p>The additional server products generally run on a Windows server. This
means that all the disadvantages of an insecure, unstable platform also
apply to the server products that run on those servers. For example,
Microsoft SQL Server is a product that has relatively few problems. It's
basically a straightforward implementation of a general SQL server,
based on technology not developed by MS but purchased from Sybase. Prior
to V7, SQL Server was mostly Sybase code. It wasn't until V7 that SQL
Server saw major rewrites.<br />
While SQL Server causes relatively few problems, it is not a very
remarkable or innovative product. Not only does it bear all worst of
Microsoft's hallmarks (things like Service Pack 4 for MS SQL Server
2000 having to mostly fix problems caused by Service Pack 3) but if I
had waited until 2005 to implement database partitioning, I think I'd
be covering it up, not trumpeting it to the world...<br />
Still SQL Server is not a bad product as far as it goes, certainly not
by Microsoft standards. However, no database service can perform better
or be more reliable than the platform it's running on. (This goes of
course for any software product, not just for a database server.) All
vulnerabilities that apply to the Windows server directly apply to the
database service as well.</p>

<p>Other additional server products come with their own additional problems.
Microsoft's webserver product, Internet Information Server (IIS) is designed
not just to serve up web pages written in the standard HTML language, but
also to provide additional authentication and links to content databases,
to add server and client side scripting to web pages, to generate Dynamic
HTML and Active Server Pages, et cetera. And it does all these things, and
more, but often not very well. IIS is outperformed by all other major
webserver products (especially Apache). IIS' authentication is far from robust
(the general lack of security in MS products is discussed below) and the
integration of an IIS webserver with a content database server is far from
seamless. Dynamic HTML, ASP and scripting require the webserver to execute
code at the server end, and there Microsoft's bad process management comes
into play: server load is often excessive. Running code at the server end
in response to web requests creates a lot of security issues as well, and
on top of all that the web pages that are generated do not conform to the
global HTML standards, they are only viewed correctly in Microsoft's own
web browser products.</p>

<p>Microsoft's mail server product, Exchange, has a few sharp edges as well.
To begin with, its performance is definitely sub-standard. Where one
Unix-based mail server will easily handle thousands of users, an Exhange
server maxes out at about one hundred. So to replace large Unix-based email
services with Exchange generally requires a server farm.<br />
A much bigger problem is Exchange's lack of stability and reliability.
To lose a few days worth of corporate E-mail in an Exchange crash is not
uncommon. Most of these problems are caused by the hackish quality of the
software. Exchange is designed to integrate primarily with other Microsoft
products (especially the Outlook E-mail client) and it doesn't take the
Internet's global RFC standards too seriously. This limits compatibility
and may cause all kinds of problems. Outlook Express also has a strange
way of talking IMAP to the Exchange server. It makes a bazillion IMAP
connections; each connection logs in, performs one task, sets the connection
to IDLE-- and then drops the connection. Since OE does not always close the
mailbox properly before dropping the connection, the mailbox and Outlook do
not necessarily sync up. This means that you may delete messages in OE that
magically return to life in a few minutes because those deletions did not get
disseminated to the mailbox before the connection terminated.</p>

<p>Just like other Microsoft applications, the additional server products are
tightly integrated into Windows during installation. They replace DLLs
belonging to the operating system, and they run services at the system level.
This does not improve the stability of the system as a whole to begin with,
and of course most of the code in the additional server products is of the
same doubtful quality as the code that makes up Windows. The reliability and
availability of any service can never be better than the server OS it runs
on. However most of Microsoft's additional server products add their own
complexity, bugs and glitches to the system, which only makes it worse.
The resulting uptime and reliability figures are rather predictable. The
inefficiency that exists in Windows is also present in the additional server
products, so as a rule of thumb each additional service needs its own server
platform. In other words: if you need a file and print server, a web server
and a mail server, you need three separate systems whereas Unix or Netware
could probably do the job on only one system.</p>

<h3>Desktop: bigger but not better</h3>
<p>Microsoft desktop applications (like Word and Excel) are largely more of
the same. They're in the terminal stages of feature bloat: they're full of
gadgets that don't really add anything useful to the product, but that ruin
productivity because of their complexity, and that introduce more errors,
increase resource demands, and require more code which in turn leads to
increased risks. After years of patching and adding, the code base for these
products has become very messy indeed. Security, if any, has been added as an
afterthought here, too. For example, a password-protected Word document is not
encrypted in any way. Inserting a 'protected' document into another
non-protected document (e.g. an empty new document) is enough to get around
the 'protection'. And if that fails, a simple hex editor is enough to change
the 'Password To Modify' in a document. Microsoft is aware of this, but now
claims that the 'Password To Modify' is only intended to &quot;prevent
accidental changes to a document&quot; and not to offer protection from
modifications by malicious third parties. Uh-huh.</p>

<p>Animated paper clips don't really make Word a better word processor. We'd be
better off with other things, such as a consistent behavior of the auto-format
features, the ability to view markup codes, or a more complete spell checking
algorithm and dictionary. But in spite of all the &quot;new&quot; versions of
Office and persistent feature requests from their users, Microsoft still
hasn't gotten around to that. Instead we have multi-language support that
tends to 'forget' its settings occasionally, and an 'auto-correct' feature
that's limited to the point of being more annoying than useful. Word documents
have become excessively large and unwieldy, and occasionally they are
corrupted while being saved to disk. When that happens, Word cannot recover
these documents and will crash in the attempt to open them.</p>

<p>In fact it's hilarious that the latest version of Office, well into the 21st
century, still can't handle multiple users reading and writing the same data.
It's stuck in the eighties, when multiple users might have been able to read
the same data, but all but the best systems couldn't properly handle writing
to the same files, let alone database records. This problem, referred to as
record locking, was fixed in modern software over a decade ago.</p>

<p>We can be brief about Excel: it has similar problems, and calculation errors
in formula-based spreadsheets on top of that. Excel is full of frills and
spiffy graphics and animations, but essentially it's still a spreadsheet that
cannot count and that requires many formulas and macros to be rewritten for
each new version of Excel.</p>

<p>The database component of Office, Microsoft Access, isn't exactly
a stellar piece of work either. Access, apart from its quirky way of
interfacing with backend databases, can still lock out an entire (possibly
mission-critical) database, just because one user hasn't shut down the
application used to write or modify data. Access is actually supposed
to be able to properly handle this condition, but it doesn't. And in a
stunning display of lack of understanding, Access 2007 introduced the
use of multi-valued data types in SQL databases, in an attempt to make
the product easier for power users to drive. The development team felt
that power users find the creation of many-to-many joins using three
tables conceptually very difficult, and will find multi-valued data
types a much easier solution. In this they are correct; users certainly
do struggle with the concept of creating many-to-many joins using three
tables as is the 'classic' way in SQL. However the reason for doing it
the old-fashioned way is that this is totally accurate and predictable,
and that every bit of data (every atomic value) will always be accessible,
which was one of main design principles (perhaps even the whole point)
of SQL's design around atomic values. The multi-valued approach is like
putting cruise control on a back hoe or a bullldozer in an attempt to
make it easier for unskilled operators to use, and it will result in a
similar mess.</p>

<p>Menu interfaces in all Microsoft applications, even in those that are bundled
in MS Office, are inconsistent and non-intuitive. For example, the menu
option to set application preferences, which may be titled 'Preferences' in
some products but 'Options' in others, may be found under the 'File' menu,
under the 'Edit' menu, under 'View' or somewhere else, depending on what
product you're currently using. To create even more confusion, the same options
in different applications do not work identically. For example the 'Unsorted
list' button (to create a bullet list) handles indentation correctly in Word
but ignores it completely in PowerPoint (PowerPoint adds bullets but messes
up the left outline of the text). And the 'F3' key activates the 'search
again' function for string searches in practically all Microsoft products,
<i>except in Internet Explorer and Excel</i> where it brings up something
totally different for no apparent reason.</p>

<h3>Microsoft does the Internet</h3> <!-- Like Debbie does Dallas -->
<p>Microsoft's most important application outside MS Office is without doubt
Internet Explorer. In its first incarnation IE was a very unremarkable web
browser; e.g. version 2.0 as it was shipped with Windows NT 4 was so backward
that it didn't even have frame capability. This soon changed as Microsoft
began integrating the web browser with Windows as a part of their integration
and bundling strategies (which are discussed in detail below).</p>

<p>In all honesty it must be said that recent versions of IE (starting
with version 6) aren't really bad web browsers. They mostly do what
they're supposed to do. True, there are a few annoying problems. IE has
more than its share of annoying bugs in style sheets and some strange
discrepancies in the rendering of tables. It also tends to become confused
when you submit a page that contains more than one button, erroneously
returning values for multiple button names. It has its own ideas about
how the Domain Object Model (DOM) should behave, boasts Jscript behavior
that is different from any other browser, lacks proper support for xHTML
support and character encoding negotiations. And of course PNG support
in IE6 is so bad that it has singlehandedly delayed the acceptation of
this image format by many years, and perhaps forever.</p>

<p>Even so, on the whole IE6 and 7 do the job well enough for most
users. At least they display standards-compliant HTML as more or
less correctly rendered web pages, at a speed that is by all means
acceptable. Previous versions of IE weren't nearly this good, and even
contained deliberate deviations from the global HTML standards that
were intended to discourage the use of standardized HTML in favor of
Microsoft's own proprietary and restrictive ideas.</p>

<p>The main drawbacks of Internet Explorer lie in the fact that it tries to be
more than just a web browser. It adds scripting support (with the ability to
run Visual Basic or Jscripts that are embedded in web pages) and it hooks
directly into the Windows kernel. I've seen web pages that would shut down the
Windows system as soon as the page was viewed with Internet Explorer. Microsoft
doesn't seem to have bothered very much with basic security considerations,
to put it mildly. And of course the installation of a new version of Internet
Explorer replaces (overwrites) significant portions of the Windows operating
system, with all the drawbacks discussed above.</p>

<p>Similar problems are found in Outlook, Microsoft's E-mail client. Outlook is
in fact a separate application, but it isn't shipped separately. There are two
versions: one is bundled with Internet Explorer (this version is called Outlook
Express) and the other is part of MS-Office (this version is named 'Outlook'
and comes with groupware and scheduler capabilities). In itself Outlook is an
acceptable, if unremarkable, E-mail client; it allows the user to read and
write E-mail. It comes with a few nasty default settings, but at least these
can be changed, although the average novice user of course never does that.
(For example, messages are sent by default not as readable text but as
HTML file attachments. When a user replies to an E-mail, the quoting feature
sometimes uses weird formatting that won't go away without dynamite. And
there's often a lot of junk that accompanies an outgoing E-mail message.)
More serious is the fact that both Outlook and its server-end companion
Exchange tend to strip fields from E-mail headers, a practice that is
largely frowned upon. This also makes both network administration and
troubleshooting more difficult.</p>

<p>The most worrying problem with Outlook is that it comes with a lot
of hooks into Internet Explorer. IE code is being used to render HTML file
attachments, including scripts that may be embedded into an HTML-formatted
E-mail message. Again Microsoft seems to have been completely unaware of the
need for any security here; code embedded in inbound E-mail is by default
executed without any further checking or intervention from the user.</p>

<h3>Basic insecurity of MS products</h3>
<p>Which brings us to another major weakness of all Microsoft products:
security, or rather the lack thereof. The notorious insecurity of Microsoft
software is a problem in itself.</p>

<p>It all begins with Windows' rather weak (not to say naive) security
models, and it's apalling quality control..  The number of reports
on security holes has become downright embarrassing, but it still
keeps increasing regularly. On the other hand, Windows security holes
have become so common that they hardly attract attention anymore.
Microsoft usually downplays the latest security issues and releases
another patch... after the fact.  If Microsoft really wanted to resolve
these software problems, they would take greater care to ensure such
problems were fixed before its products went on sale-- and thus reverse
the way it traditionally conducts business. Doing so would mean less
resources wasted by its customers each year patching and re-patching
their systems in an attempt to clean up after Microsoft's mistakes, but
it would also decrease the customers' dependency on what Microsoft calls
'software maintenance'.</p>

<p>In the meantime, hackers are having a ball with Microsoft's shaky
security models and even weaker password encryption (which includes
simple XOR bitflip operations, the kind of 'encryption' that just about
every student reinvents in school). Hackers, script kiddies and other
wannabees get to take their pick from the wide choice of elementary
security weaknesses to exploit. Some historic and highly virulent worms,
for example, spread so rapidly because they could crack remote share
passwords in about twenty seconds. This did not stop Microsoft from
running an advertising campaign in spring 2003 that centered on hackers
becoming extinct along with the dodo and the dinosaur, all because of
Microsoft's oh so secure software. Unsurprisingly this violated a few
regulations on truth in advertising, and the campaign had to be hastily
withdrawn.</p>

<p>In an attempt to clean up their image somewhat, Microsoft made sure
that Windows Vista was launched with a lot of security-related noise. For
starters, Vista has a better set of default access privileges. Well,
finally! Ancient commercial OSes like Univac Exec, CDC Scope and DEC VMS
all had special accounts with various permissions ages ago as a matter of
course and common sense. On Windows every user needed administrator rights to
do basic tasks. But apart from being decades too late, this basic requirement
has been met in a typical Microsoft fashion: it creates a security hole
so large that it might more properly be called a void. In Windows Vista the
need for certain access privileges are now tied to... program names! For
example, if Vista sees that an application developer has created a Microsoft
Visual C++ project with the word &quot;install&quot; in the project name,
then that executable will automatically require admin rights to run. Create
exactly the same project but call it, say, Fred, and the need for elevated
access permissions magically disappears. In short, all that malicious software
has to do is to present a harmless-looking name to Vista, and Vista will let
it through.</p>

<p>Apart from the fact that proper access control should have been implemented
in Windows NT right from the start, and that Microsoft botched it when it
finally did appear in Vista, the rest of Vista's security is the usual
hodge-podge of kludges and work-arounds that often attempt to patch one hole
and create another one in the process. For example let's look at Vista's
&quot;PatchGuard&quot; service. PatchGuard crashes the computer when it
detects that specific internal data structures have been &quot;hooked&quot;,
which is a common way that malicious software starts doing its damage. Not only
does this work-around still not amount to proper protection of operating system
code in the first place, but it also prevents third-party security products
(e.g. anti-virus and anti-spyware programs) from working correctly. In
order to remedy this, Microsoft released API's that <i>essentially enable
a user-level program to shut down Vista's Security Center</i>. Uh-huh.</p>

<p>An important part of the problem is Windows' lack of proper separation
between code running on various system and user levels. Windows was designed
around the basic assumption that code always runs with the highest privilege,
so that it can do almost anything, including malicious intent. This makes it
impossible to prevent malicious code from invading the system. Users may
(inadvertently or deliberately) download and run code from the Internet, but
it's impossible to adequately protect system level resources from damage by
user level code.</p>

<h3>Integrated vulnerabilities</h3>
<p>The tight integration between the various Microsoft products does little to
improve overall security. All software components are loaded with features,
and all components can use each other's functions. Unfortunately this means
that all security weaknesses are shared as well. For example, the Outlook
E-mail client uses portions of Internet Explorer to render HTML that is
embedded in E-mail messages, including script code. And of course IE and
Outlook hook into the Windows kernel with enough privileges to run
arbitrary malicious code that happens to be embedded in a received
E-mail message or a viewed web page. Since Outlook uses portions of
IE's code, it's vulnerable to IE's bugs as well. So a scripting
vulnerability that exists in Outlook also opens up IE and vice versa, and if
IE has a hook into certain Windows kernel functions, those functions can also
be exploited through a weakness in Outlook. In other words, a minor security
leak in one of the components <i>immediately puts the entire system at
risk.</i> Read: a vulnerability in Internet Explorer means a vulnerability in
Windows Server 2003! A simple Visual Basic script in an E-mail message has
sufficient access rights to overwrite half the planet, as has been proven by
Email virus outbreaks (e.g.  Melissa, ILOVEYOU and similar worms) that have
caused billions of dollars worth of damage.</p>

<p>A good example are Word viruses; these are essentially VBS (Visual
Basic Script) routines that are embedded in Word documents as a macro. The
creation of a relatively simple macro requires more programming skills than
the average office employee can be expected to have, but at the same time a
total lack of even basic security features makes Word users vulnerable to
malicious code in Word documents. Because of the integrated nature of the
software components, a Word macro is able to read Outlook's E-mail
address book and then propagate itself through the system's E-mail
and/or networking components. If Windows' security settings prevent this,
the malicious virus code can easily circumvent this protective measure by the
simple expedient of <i>changing the security settings</i>. How's that for
security?</p>

<p>Similarly, VBS scripts embedded in web pages or E-mail messages may exploit
weaknesses in IE or Outlook, so that viewing an infected web page or receiving
an infected E-mail is enough to corrupt the system without any further action
from the user (including manually downloading a file or opening an attachment).
Through those weaknesses the malicious code may access data elsewhere on the
system, modify the system's configuration or even start processes. In March
2000, a hacker wrote (of course anonymously) on ICQ:</p>

<blockquote><i>
  21/03/2k: Found the 1st Weakness:  In Windows 2000 [...] there is a Telnet
  daemon service, which is not started by default. It can be remotely started
  by embedding a COM object into HTML code that can be posted on a web
  page, or sent to an Outlook client. Following script will start the Telnet
  service:<br />
  <tt>&lt;SCRIPT LANGUAGE=VBScript&gt;
    CreateObject(&quot;TlntSvr.EnumTelnetClientsSvr&quot;)&lt;/SCRIPT&gt;</tt>
  <br />
  We've tried it and it really works. Only a Problem... we've put it into a
  html page. When opening the page... our best friend &quot;IE5&quot; shows
  an alert msg saying that &quot;you're going to run some commands that can be
  dangerous to your PC...Continue?&quot;  We must fix it! No problem using
  Outlook... [sic]
</i></blockquote>

<p>Note that after patching no fewer than seven different security holes
in the Windows 2000 telnet code (yes, that's seven security leaks in
telnet alone!) Microsoft released another patch in February 2002,
to fix security issue number eight: another buffer overflow vulnerability.
Somehow I don't think this patch will be the last. If you don't succeed at
first, try seven more times, try, try (and try some more) again. Seen in this
light, it's  not surprising that J.S. Wurzler Underwriting Managers, one of
the first companies to offer hacker insurance, have begun charging clients 5
to 15 percent more if they use Microsoft's Windows NT software in their
Internet operations.</p>

<p>Microsoft knows exactly how bad their own product security is. Nevertheless
they wax lyrical about new features rather than accept their responsibility
for their actions. To quote Tom Lehrer:</p>

<blockquote><i>
  &quot;The rockets go up, who cares where they come down?<br />
  That's not my department, says Werner von Braun.&quot;
</i></blockquote>

<p>Microsoft can't be unaware of the risks and damages they cause. After all
they prudently refuse to rely on their own products for security, but use
third party protection instead. (See above.) And while they try to push their
user community into upgrading to new product versions as soon as possible,
Microsoft can hardly be called an early adopter. In the autumn of 2001 they
still did not run Windows and Exchange 2000 on their own mail servers yet,
long after these versions had been released to the market. On other internal
systems (less visible but still there) a similar reluctance can be seen to
upgrade to new versions of MS products. Only after many security patches
and bug fixes have been released will Microsoft risk upgrading their own
critical systems.</p>

<h3>Sloppiness makes the problem worse</h3>
<p>Many security problems are caused by the sloppy code found in many
Microsoft products. The many buffer overrun vulnerabilities can be combined
with scripting weaknesses. You don't need to open E-mail attachments or even
read an incoming E-mail message to risk the introduction of malicious code on
your system. Just receiving the data (e.g. downloading E-mail from a POP3
server or viewing a web page) is enough. Yes, stories like this have long been
urban legend, but Outlook has made it reality. Microsoft explains: &quot;The
vulnerability results because a component used by both Outlook and Outlook
Express contains an unchecked buffer in the module that interprets E-mail
header fields when certain E-mail protocols are used to download mail from
the mail server. This could allow a malicious user to send an E-mail that,
when retrieved from the server using an affected product, could cause code
of his choice to run on the recipient's computer.&quot; This vulnerability
has been successfully exploited by Nimda and other malicious worm programs.
Other worm programs (e.g. Code Red) combine vulnerabilities like this with
creatively constructed URL's that trigger buffer overruns in IIS. Even without
the Frontpage extensions installed it is relatively easy to obtain unencrypted
administration passwords and non-public files and documents from an IIS
webserver. Furthermore, this &quot;E-commerce solution of the future&quot;
contains a prank (a hardcoded passphrase deriding Netscape developers as
&quot;weenies&quot;) in the code section concerned with the access
verification mechanism for the whole system. And there are many more
weaknesses like this. The list goes on and on and on.</p>

<p>IIS is supposed to power multi-million dollar E-commerce sites, and it has
many backend features to accomplish this application. But each and every time
we hear about a large online mailorder or E-commerce website that has spilled
confidential user data (including credit card numbers) it turns out that that
website runs IIS on Windows NT or 2000. (And that goes for adult mailorder
houses too. I'm not quite sure what kind of toy a Tarzan II MultiSpeed
Deluxe is, but I can probably tell you who bought one, and to which address
it was shipped. Many E-commerce websites promise you security and discretion,
but if they run IIS they can only promise you good intentions and nothing
more. Caveat emptor!)</p>
<!-- Case in point: Wehkamp and Christine le Duc. -->

<p>The Code Red and Nimda worms provided a nice and instructive demonstration
of how easy it is to infect servers running IIS and other Microsoft products,
and use them for malicious purposes (i.e. the spreading of malicious code and
DDoS attacks on a global scale). Anyone who bothers to exploit one of the many
documented vulnerabilities can do this. Some of the vulnerabilities exploited
by Code Red and Nimda were months old, but many administrators just can't keep
up with the ridiculous amount of patches required by IIS. Nor is patching always
a solution: the patch that Microsoft released to counter Nimda contained bugs
that left mission-critical IIS production servers non-operational.</p>

<p>On 20 June 2001, Gartner vice president and analyst John Pescatore wrote:</p>

<blockquote><i>
  IIS security vulnerabilities are not even newsworthy anymore as they are
  discovered almost weekly. This latest bug echoes the very first reported
  Windows 2000 security vulnerability in the Indexing Service, an add-on
  component in Windows NT Server incorporated into the code base of Windows
  2000. As Gartner warned in 1999, pulling complex application software into
  operating system software represents a substantial security risk. More
  lines of code mean more complexity, which means more security bugs. Worse
  yet, it often means that fixing one security bug will cause one or more new
  security bugs.<br />
  The fact that the beta version of Windows XP also contains this
  vulnerability raises serious concerns about whether XP will show any
  security improvement over Windows 2000.
</i></blockquote>

<p>On 19 September 2001, Pescatore continued:</p>

<blockquote><i>
  Code Red also showed how easy it is to attack IIS Web servers [...] Thus,
  using Internet-exposed IIS Web servers securely has a high cost of
  ownership. Enterprises using Microsoft's IIS Web server software have to
  update every IIS server with every Microsoft security patch that comes
  out&nbsp;-&nbsp;almost weekly. However, Nimda (and to a lesser degree Code
  Blue) has again shown the high risk of using IIS and the effort involved in
  keeping up with Microsoft's frequent security patches.<br />
  Gartner recommends that enterprises hit by both Code Red and Nimda
  immediately investigate alternatives to IIS, including moving Web
  applications to Web server software from other vendors, such as iPlanet and
  Apache. Although these Web servers have required some security patches, they
  have much better security records than IIS and are not under active attack
  by the vast number of virus and worm writers. Gartner remains concerned that
  viruses and worms will continue to attack IIS until Microsoft has released a
  completely rewritten, thoroughly and publicly tested, new release of IIS.
  Sufficient operational testing should follow to ensure that the initial wave
  of security vulnerabilities every software product experiences has been
  uncovered and fixed. This move should include any Microsoft .Net Web
  services, which requires the use of IIS. Gartner believes that this rewriting   will not occur before year-end 2002 (0.8 probability).
</i></blockquote>

<p>As it turns out, Gertner's estimate was overly optimistic. Now, several
years later, still no adequately reworked version of IIS has been released
yet.</p>

<h3>So how serious is this?</h3>
<p>In all honesty it must be said that Microsoft has learned to react
generally well to newly discovered security holes. Although the severity of
many security problems is often downplayed and the underlying cause (flawed or
absent security models) is glossed over, information and patches are generally
released promptly and are available to the user community without cost. This is
commendable. But then the procedure has become routine for Microsoft, since
new leaks are discovered literally several times a week, and plugging leaks
has become part of Microsoft's core business. The flood of patches has become
so great that it's almost impossible to keep up with it. This is illustrated
by the fact that most of today's security breaches successfully exploit leaks
for which patches have already been released. In fact the sheer volume of
patchwork eventually became sufficient to justify the automated distribution
of patches. For recent versions of Windows there is an automatic service to
notify the user of required &quot;critical updates&quot; (read: security
patches) which may then be downloaded with a single mouseclick. This service
(which <i>does</i> work fairly well) has become very popular. And for good
reason: in the year 2000 alone MS released about 100 (yes, one hundred)
security bulletins - that's an average of one newly discovered security-related
issue <i>every three to four days!</i> The number of holes in Microsoft
products would put a Swiss cheese to shame.</p>

<p> And the pace has increased rather than slowed down. For example, once you
install a &quot;recommended update&quot; (such as Media Player) through
the Windows Update service, you discover immediately afterwards that you must
repeat the whole exercise in order to install a &quot;critical update&quot;
to patch the new security leaks that were introduced with the first download!
It's hardly reasonable to expect users to keep up with such a rat race, and
not surprising that most users can't. As a result, many E-mail viruses and
worms exploit security holes that are months or years old. The MSBlaster worm
that spread in the summer of 2003 managed to infect Windows Server 2003 using
a vulnerability that was already present in NT4!</p>

<p>In an age where smokers sue the tobacco industry for millions of dollars
over health issues, all Microsoft products had better come with a warning on
the package, stating that &quot;This product is insecure and will cause
expensive damage to your ICT infrastructure unless you update frequently and
allocate time on a daily basis to locate, download, test and install the
patch-du-jour&quot;. Unfortunately they don't, and Windows-based macro and
script viruses emerge at a rate of several hundreds a month, while the
average time for an unpatched Windows server with a direct Internet connection
to be compromised is only a few minutes.</p>
<!-- See: http://project.honeynet.org/papers/trends/life-linux.pdf --> 

<h3>Patch release as a substitute for quality</h3>
<p>An interesting side effect of the ridiculous rate with which patches have to
be released is that some users now get the impression that Microsoft takes
software maintenance very seriously and that they are constantly working to
improve their products. This is of course rather naive. If they'd bought a
car that needed serious maintenance or repairs every two weeks or so, they
probably wouldn't feel that way about their car dealer.<br />
Redmond has exploited this misconception more than once. In recent
comparisons of Windows vs. Linux they quoted patch response times, in an
attempt to show that Windows is more secure than Linux. They had of course
to reclassify critical vulnerabilities as non-critical, misinterpret a lot
of figures, and totally ignore the fact that Windows develops many times the
number of vulnerabilities than any other product.</p>

<p>Even so, if Microsoft's patching policy was effective we'd have run out
of security holes in most MS products about now, staring with IE. Obviously
no amount of patching can possibly remedy the structural design flaws in
(or absence of) Microsoft products' security. A patch is like a band-aid:
it will help to heal a simple cut or abrasion, but it won't prevent getting
hurt again, in the same way or otherwise, and for a broken leg or a genetic
deficiency it's totally useless, even if you apply a few thousand of them.
The obvious weak point in the system is of course the integration of
application software into the OS. Microsoft likes to call Windows
&quot;feature-rich&quot; but when they have to release an advisory on a serious
vulnerability in Windows Server 2003 that involves MIDI files, it becomes
obvious that the set of &quot;features&quot; integrated in Windows has long
since passed the limits of usefulness.</p>

<h3>Microsoft's solution: security through obscurity</h3>
<p>Lately Microsoft lobbyists are trying to promote the idea that free
communication about newly discovered security leaks is not in the interest
of the user community, since public knowledge of the many weaknesses in their
products would enable and even encourage malicious hackers to exploit those
leaks. Microsoft's Security Response Center spokesman Scott Culp blamed
security experts for the outbreak of worms like Code Red and Nimda, and in an
article on Microsoft's website in October 2001 he proposed to restrict
circulation of security-related information to &quot;select circles&quot;.
And it's all for our own good, of course. After all, censorship is such a
nasty word.</p>

<p>In August 2002, during a court hearing discussing a settlement between
Microsoft and the DoJ, Windows OS chief Jim Allchin testified how cover-ups
are Microsoft's preferred (and recommended) course of action:</p>

<blockquote><i>
  &quot;There is a protocol dealing with software functionality in Windows
  called message queueing, and there is a mistake in that protocol. And that
  mistake, if we disclosed it, would in my opinion compromise a company who
  is using that particular protocol.&quot;
</i></blockquote>

<p>In the meantime things are only getting worse with the lack of security
in Microsoft products. The latest incarnation of Office (Office XP) provides
full VBA support for Outlook, while CryptoAPI provides encryption for
messages and documents, <em>including VBS attaches and macro's</em>. In other
words, anti-virus software will no longer be able to detect and intercept
viruses that come with E-mail and Word documents, rendering companies
completely defenseless against virus attacks.</p>

<p>Clearly this is a recipe for disaster. It's like a car manufacturer who
floods the market with cars without brakes, and then tries to suppress all
consumer warnings in order to protect his sales figures.</p>

<h3>Count the bugs: 1 + 1 = 3</h3>
<p>Another  worrying development is that leaky code from products such as IIS or
other products is often installed with other software (and even with Windows
XP) without the system administrators being aware of it. For example: SQL
Server 2000 introduced 'super sockets' support for data access via the
Dnetlib DLL. It provides multi-protocol connectivity, encryption, and
authentication; in other words a roll-up of the different implementations of
these technologies in past versions of the product. A system would only have
this DLL if SQL Server 2000, the client administration tools, MSDE, or a
vendor-specific solution was installed on the box. However, with XP this DLL
is part of the default installation-- even on the home edition. One has to
wonder how a component goes from &quot;installed only in specialized machines
on a particular platform&quot; to &quot;installed by default on all flavors
of the OS.&quot; What other components and vulnerabilities are now
automatically installed that we don't know about?</p>

<p>And the Windows fileset is getting extremely cluttered as it is. Looking
through the WINNT directory on a Windows 2000 or XP system, you'll find lots
of legacy executables that are obsolete and never used: Media Player 5,
16-bit legacy code from previous Windows versions as far back as version 3.10
(in fact the bulk of the original Windows 3.10 executable code is there),
files that belong to features that are never used in most cases (e.g. RAS
support) or accessibility options that most users fortunately don't need
(such as the Narrator and Onscreen Keyboard features). Dormant code means
possible dormant security issues. The needless installation of such a roundup
reflects the laziness of Microsoft's developers: if you just install
everything but the kitchen sink, you can just assume it's there at a later
time and not bother with the proper verifications. Of course this practice
doesn't improve quality control at all, it merely adds to the bloat that
has plagued Windows from day one.</p>

<h3>Expect no real improvement</h3>
<p>The future promises only more of the same. Since Microsoft is always
working on the next versions of Windows, it seems a safe assumption that
we're stuck with the current flawed Windows architecture and that no
structural improvements are to be expected. So far Microsoft has never
seemed capable of cleaning up their software architectures. Instead they
concentrate on finding workarounds to avoid the real problem.</p>

<p>A prime example was Microsoft's recommendation that PCs &quot;designed
for Windows XP&quot; should no longer accept expansion cards but only
work with USB peripherals. The reason for this recommendation was that
XP and its successor Vista still suffer from the architecture-related
driver problems that have caused so many Windows crashes in the past. In
an attempt to get rid of the problem, Microsoft tried to persuade PC
manufacturers to abandon the PCI expansion bus. The fact that this
recommendation was immediately rejected by the hardware industry is
irrelevant; the point is that Microsoft tried to get rid of expansion bus
support rather than improve Windows' architecture to make it robust. When
this attempt failed, they resorted to their second option, which was to
discourage (in XP) and prevent (in Vista) the user from installing
any device driver that hasn't been tested and certified not to cause
OS instabilities.</p>

<p>In fact Microsoft <i>does</i> recognize the need for structural
improvements in Windows' architecture. However, in spite of years
of effort it has proven impossible for them to deliver any. This is
perfectly illustrated by the early announcements of the .Net initiative,
shortly after the turn of the century. Eventually .Net materialized as
a framework for programmers to facilitate the development of network
applications. During the first year or so of its initial announcement,
though, .Net was presented as the future of desktop computing that was
going to solve all deficiencies in Windows once and for all. Its most
touted &quot;innovation&quot; was &quot;Zero Impact Install&quot; which
amounted to doing away with the tight integration between application
and operating system. Instead of the current mess of DLL's being
inserted into the OS and settings spread throughout an insecure registry
database, applications would live in their own subdirectories and be
self-contained. Code would be delivered in a cross-platform format and
be JIT-compiled (Just In Time) for the platform it was to run on.</p>

<p>While these things would have meant a dramatic improvement over the
current situation, their innovation factor was of course close to zero:
Windows' need for an adequate separation between OS and application
code makes sophisticated ICT professionals long for Unix, mainframe
environments or even DOS. JIT-compilation is nothing new either;
it wasn't even a new idea when Sun Microsystems proposed Java in
the mid-1990's. But more importantly, none of these changes ever
materialized. Windows XP and Vista were going to be the first step to
accomplish this enlightened new vision, but after the release of both
Windows versions no trace of these bold plans remains. Instead the
release of Vista was delayed by several years, because it was impossible
to make even minor improvements without massive code rewrites. After
Vista it's clearer than ever that no significant structural improvements
of Windows will ever be forthcoming.</p>

<h3>Trustworthy computing? Not from Microsoft</h3>
<p>Lately Microsoft has been making a lot of noise about how they now
suddenly take security very seriously, but the bad overall quality of
their product code makes it impossible to live up to that promise. Their
Baseline Security Analyzer (which they released some time ago as part of
their attempts to improve their image) was a good indication: it didn't
scan for vulnerabilities but merely for missing patches, and it did a
sloppy job at that with a lot of false positives as a result.</p>

<p>Let's face it: Microsoft's promises about dramatic quality improvement are
unrealistic at best, not to say misleading. They're impossible to fulfil in
the foreseeable future, and everyone at Microsoft knows it. To illustrate,
in January 2002 Bill Gates wrote in his &quot;Trustworthy computing&quot;
memo to all Microsoft employees:</p>

<blockquote><i>
  &quot;Today, in the developed world, we do not worry about electricity and
  water services being available. With telephony, we rely both on its
  availability and its security for conducting highly confidential business
  transactions without worrying that information about who we call or what
  we say will be compromised. Computing falls well short of this, ranging
  from the individual user who isn't willing to add a new application because
  it might destabilize their system, to a corporation that moves slowly to
  embrace e-business because today's platforms don't make the grade.&quot;
</i></blockquote>

<p>Now, for &quot;today's platforms&quot; read &quot;a decade of Windows,
in most cases&quot; and keep in mind that Microsoft won't use their own
security products but relies on third party products instead. Add to
that the presence of spyware features in Windows Media Player, Internet
Explorer 7 and XP's Search Assistant (all of which contact Microsoft
servers regularly whenever content is being accessed), the fact that
Windows XP Home Edition regularly connects to a Microsoft server for
no clearly explained reason, and the hooks for the Alexa data gathering
software in IE's 'Tools/Show Related Links' feature... and the picture
is about complete. Trustworthy? Sure! Maybe Big Brother isn't watching you,
and maybe nothing is being done with the information that's being collected
about what you search for and what content you access... and maybe
pigs really can fly. For those who still don't get it: in November
2002 Microsoft made customer details, along with numerous confidential
internal documents, freely available from a very insecure FTP server. This
FTP server sported many known vulnerabilities, which made gaining access
a trivial exercise. Clearly, Microsoft's recent privacy-concerned and
quality-concerned noises sound rather hollow at best. They don't even
have any respect for their customers' privacy and security themselves.</p>

<p>As if to make a point, a few weeks after Gates' memo on Trustworthy
Computing, Microsoft managed to send the Nimda worm to their own Korean
developers, along with the Korean language version of Visual Studio .Net, thus
spreading an infection that had originated with the third-party Korean
translators. How 'trustworthy' can we expect a company to be, if they aren't
even capable of basic precautions such as adequate virus protection in their
own organisation?</p>

<p>Of course nothing has changed since Gates wrote the above memo. Security
holes and vulnerabilities in all MS products, many of which allow blackhat
hackers to execute arbitrary code on any PC connected to the Internet, continue
to be discovered and exploited with a depressing regularity. Microsoft claims
to have put 11,000 engineers through security training to solve the problem,
but all users of Microsoft products continue to be plagued by security flaws.
It's obvious that real improvement won't come around anytime soon. Windows
Server 2003 ws marketed as &quot;secure by design&quot; but apart from
a couple of improved default settings and the Software Restriction Policies
not much has changed. Right after Windows Server 2003 was released, its
first security patch (to plug a vulnerability that existed through Internet
Explorer 6) had to be applied, to nobody's surprise.</p>

<h3>Lip service will do</h3>
<p>Following Gates' memo on Trustworthy Computing, Microsoft has made a lot
of noise about taking security very seriously. However, Stuart Okin, MS
Security Officer for the UK, described security as &quot;a recent issue&quot;.
During an interview at Microsoft's Tech Ed event in 2002, Okin explained that
recent press coverage on viruses and related issues had put security high on
Microsoft's agenda. Read: it was never much of an issue, but now it's time to
pay lip service to security concerns in order to save public relations.</p>

<p>And indeed Microsoft's only real 'improvement' so far has been an
advertising campaign that touts Windows XP as <i>the</i> secure platform that
protects the corporate user from virus attacks. No, really&nbsp;- that's what
they said. They also made a lot of noise about having received
&quot;Government-based security certification&quot;. In fact this only means
that Windows 2000 SP3 met the CCITSE Common Criteria, so that it can be
part of government systems without buyers having to get special waivers from
the National Security Agency or perform additional testing every time.
CC-compliance does <i>not</i> not mean the software is now secure, but merely
means the testing has confirmed the code is working as per specifications.
That's all&nbsp;-- the discovery of new security holes at least once a week has
nothing to do with it. But even so, Windows 2000 SP3 was the first Microsoft
product ever that worked well enough to be CC-certified. Go figure.</p>

<p>Gates' initial launch of the Trustworthy Computing idea was much like the
mating of elephants. There was a lot of trumpeting and stamping around the
bush, followed by a brief moment of activity in high places, and then
nothing happened for almost two years. Eventually Steve Ballmer made the
stunning announcement that the big security initiative will consist of... a
lot of minor product fixes (<i>yes, again</i>), training users, and rolling up
several minor patches into bigger ones. Microsoft's press release actually
used the words, quote, &quot;improving the patch experience&quot;, unquote.
So far this &quot;improvement&quot; has mainly consisted of monthly patch
packages, which had to be re-released and re-installed several times a month
in a 'revised' monthly version more than once.</p>

<p>Another sad aspect of Microsoft's actual stance on security is neatly
summed up by Internet.com editor Rebecca Lieb, who investigated Microsoft's
commitment on fighting the epidemic flood of spam. She concludes:</p>

<blockquote><i>
&quot;[Microsoft] executives are certainly committed to saying they are
[committed to helping end the spam epidemic]. These days, Bill Gates is
front and center: testifying before the Senate; penning a Wall Street
Journal editorial; putting millions up in bounty for spammer arrests;
building a Web page for consumers; and forming an Anti-Spam Technology
&amp; Strategy Group, &quot;fighting spam from all angles-- technology,
enforcement, education, legislation and industry self-regulation.&quot;<br />
When I meet members of that group, I always ask the same question. Every
version of the Windows OS that shipped prior to XP's release last year is
configured --by default-- as an open relay. Millions have been upgraded to
broadband. Ergo, most PCs on planet Earth emit a siren call to spammers:
&quot;Use me! Abuse me!&quot; Why won't Microsoft tell its millions of
registered customers how to close the open relay?&quot;
</i></blockquote>

<p>True enough, in 2004 over 75% of all spam is distributed via Windows PCs
(on DSL and cable Internet connections) that have been compromised by email
worms and Trojan Horse infections. But rather than fix the vulnerabilities in
their products, Microsoft so far has concentrated on high-profile actions such
as a collaboration with the New York State Attorney General and a highly
publicized crusade against Internet advertising companies. Bill Gates'
reckless prediction that in the year 2006 the spam problem would be solved
has only served to demonstrate the value of Microsoft's promises on quality
and security.</p>

<p>Neither is Microsoft's own implementation of Sender Policy Framework even
remotely effective. Microsoft touted their use of SPF as a significant step in
spam reduction, and introduced it with so much fanfare that you'd think they'd
developed it themselves. However, Security appliance firm CipherTrust soon
found that spammers adopted the new standard for email authentication much
faster than legitimate emailers, and shortly after its introduction more spam
than legitimate email was sent using Sender Policy Framework. While this is
going on, implementors are balking at MS's licensing policy for the Sender ID
system, which amounts to creating great dependency on Microsoft's permission
to (continue to) use Sender ID.</p>

<p>Meanwhile Microsoft is branching into new markets and so far runs true to
form. They have already shipped their first cellphone products. Orange, the
first cellnet operator to run Microsoft Smartphone software on their SPV
phones, has already had to investigate several security leaks. On top of that
the phones are reported to crash and require three subsequent power-ups to get
going again, call random numbers from the address book and have flakey
power management. I shudder to think what will happen when their plans on the
automotive software market begin to materialize.</p>

<h3>Bottom line: things are bad</h3>
<p>In spite of what Microsoft's sales droids would have us believe, the facts
speak for themselves: developments at Microsoft are solely driven by business
targets and not by quality targets. As long as they manage to keep up their
$30 billion plus yearly turnover, nobody's posterior is on the line no matter
how bad their software is.</p>

<p>Microsoft products are immature and of inferior quality. They waste
resources, do not offer proper options for administration and maintenance,
and are fragile and easily damaged. Worse, new versions of these products
provide no structural remedy, but are in fact point releases with bugfixes,
minor updates and little else but cosmetic improvement. Recent versions of
Microsoft products are only marginally more secure than those that were
released years ago. In fact, if it weren't for additional security products
such as hardware-based or Unix-based filters and firewalls, it would be
impossible to run an even remotely secure environment with Windows.</p>

<p>MS products are bloated with an almost baroque excess of features, but that's
not the point. The point is that they are to be considered harmful, lacking
robustness and security as a direct result of basic design flaws that are in
many cases over a decade old. They promise to do a lot, but in practice they
don't do any of it very well. If you need something robust, designed for
mission-critical applications, you might want to look elsewhere. Microsoft's
need for compatibility with previous mistakes makes structural improvements
impossible. The day Microsoft makes something that doesn't suck, they'll be
making vacuum-cleaners.</p>

<p>Besides, 63,000 known defects in Windows should be enough for anyone.</p>


<hr />
<div align="right">
<a href="IhateMS.html" target="_self">Table Of Contents</a> |
<a href="IhateMS_1.html" target="_self">Previous chapter</a> |
<a href="IhateMS_3.html" target="_self">Next chapter</a>
</div>
<hr />


<p align="right"><small>Comments? <a href="../../contact.php"
  target="_self">E-mail me!</a></small></p>

<!-- Footer -->

<p align="right"><small>
<script language="javascript" type="text/javascript">
<!-- Hide
// Javascript history backlink
  document.write("<a href=\"javascript:history.back(1)\" target=\"_self\">\n");
  document.write("Back<\/a> | \n");
// Unhide -->
</script>
<a href="../../index.html" target="_top">Home</a>
</small></p>

<div class="small" style="text-align: center">
  Contents copyright &#169; 1997-2007 F.W. van Wensveen - all rights
  reserved.<br />
</div><br />

</body>
</html>
